{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import random as random_\n",
    "import re\n",
    "from glob import iglob\n",
    "\n",
    "from typing import List, Dict, Tuple, Set, Any, TypedDict, Union, Optional, Literal, Callable\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE = \"../../dataset-tools/python/out\"\n",
    "MFCCS  = f\"{DATASET_BASE}/mfccs\"\n",
    "LABELS = f\"{DATASET_BASE}/labels.json\"\n",
    "SUBGENERES = f\"{DATASET_BASE}/subgeneres.json\"\n",
    "CHECKPOINT = \"./checkpoint/saved_model.h5\"\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "SEED = random_.randint(0, 100)\n",
    "random = random_.Random(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Declaration and other Dicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class N(TypedDict): # A number \n",
    "    N: Union[float, int]\n",
    "\n",
    "class S(TypedDict): # A string\n",
    "    S: str\n",
    "\n",
    "class L(TypedDict): # A list of strings\n",
    "    L: List[S]\n",
    "\n",
    "class Track(TypedDict):\n",
    "    # IDs\n",
    "    PK: S\n",
    "    mbid: S\n",
    "\n",
    "    # Genres: \n",
    "    genre: S\n",
    "    subgenres: L\n",
    "    otherSubgenres: Optional[L]\n",
    "\n",
    "    # Moods\n",
    "    aggressive: Optional[N]\n",
    "    happy: Optional[N]\n",
    "    party: Optional[N]\n",
    "    acoustic: Optional[N] \n",
    "    electronic: Optional[N]\n",
    "    sad: Optional[N]\n",
    "    relaxed: Optional[N]\n",
    "\n",
    "    # Metadata\n",
    "    preview: S\n",
    "    name: Optional[S]\n",
    "    artist: Optional[S]\n",
    "    popularity: Optional[N]\n",
    "    album: Optional[S]\n",
    "    \n",
    "\n",
    "class LabelsJson(TypedDict):\n",
    "    tracks: Dict[str, Track]\n",
    "\n",
    "class Mfcc(TypedDict):\n",
    "    mfccs: np.ndarray\n",
    "    track_id: str\n",
    "    splits: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path:str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads a json file \n",
    "\n",
    "    Args:\n",
    "        path (str): json file path\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary indexed by a string\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgenres(parent_genre: str, subgenres: List[str]):\n",
    "    \"\"\"Get all subgenres of a given genre\n",
    "\n",
    "    Args:\n",
    "        parent_genre (str): \n",
    "        subgenres (List[str]): List of subgenres to match with a parent genre\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of subgenres\n",
    "    \"\"\"\n",
    "    return list(filter(lambda s: parent_genre in s.split(\"---\")[0] ,subgenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfccs(subgenres_list: List[str], subgenres: Dict[str, List[str]], search_path: str = MFCCS):\n",
    "    \n",
    "    track_set: Set[str] = set() # List of track ids among all subgenres_list subgenres\n",
    "\n",
    "    track_splits: List[Mfcc] = [] \n",
    "\n",
    "    for g in subgenres_list:\n",
    "        track_set.update(subgenres[g])\n",
    "\n",
    "    for npy in iglob(search_path + '/**/*.npy', recursive=True):\n",
    "        match = re.search(r\"[a-zA-Z0-9]+.npy\", npy)\n",
    "\n",
    "        if (match and match.group(0)):\n",
    "            track_id = match.group(0).replace(\".npy\", \"\")\n",
    "            if track_id in track_set: \n",
    "                try:\n",
    "                    mfccs_splits = np.load(npy)\n",
    "                    track_splits.append({\"mfccs\": mfccs_splits, \"track_id\": track_id, \"splits\": len(mfccs_splits)})\n",
    "                except IOError:\n",
    "                    print(f\"File {npy} not found\")\n",
    "\n",
    "    return track_splits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val(ds: List[Mfcc], test=0.15, val=0.15):\n",
    "    # TRAIN     \n",
    "    train_slice = floor(len(ds) * (1 - val - test))\n",
    "    train = ds[:train_slice]\n",
    "\n",
    "    rest = ds[train_slice:]\n",
    "    rest_slice =  floor(len(rest) * (1 - (test / ( test + val))))\n",
    "\n",
    "    # TEST\n",
    "    test = rest[:rest_slice]\n",
    "\n",
    "    # VAL\n",
    "    val = rest[rest_slice:]\n",
    "\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get(\"accuracy\") or 0) > 0.99:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "stop_callback = StopCallback()\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    CHECKPOINT,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackDataGen(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, data: List[Mfcc],\n",
    "                 target_f: Callable[[Track], Any],\n",
    "                 labels: LabelsJson,\n",
    "                 batch_size = BATCH_SIZE):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        X: List[np.ndarray] = []\n",
    "        Y: List[str] = []\n",
    "\n",
    "        for mfcc_ in data:\n",
    "            track = labels[\"tracks\"][mfcc_[\"track_id\"]]\n",
    "            target = target_f(track)\n",
    "            for split in mfcc_[\"mfccs\"]:\n",
    "                X.append(split)\n",
    "                Y.append((np.array(target) * mfcc_[\"splits\"]))\n",
    "\n",
    "        X_np = np.array(X)\n",
    "        X_np = np.expand_dims(X_np, axis=3)\n",
    "        Y_np = np.array(Y)\n",
    "\n",
    "        self.X, self.Y = self.unison_shuffled_copies(X_np, Y_np)\n",
    "        \n",
    "    @staticmethod\n",
    "    def unison_shuffled_copies(a, b):\n",
    "        assert len(a) == len(b), f\"len(a) = {len(a)} != len(b) = {len(b)}\"\n",
    "        p = np.random.permutation(len(a))\n",
    "        return a[p], b[p]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.Y[idx * self.batch_size:(idx + 1) * self.batch_size] \n",
    "        \n",
    "        return batch_x, np.array(batch_y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(type: Literal[\"multiclass\", \"multilabel\", \"mood\"], labels: int):\n",
    "\n",
    "\n",
    "    if type == \"multilabel\":\n",
    "        activation, loss, metrics = \"sigmoid\", \"binary_crossentropy\", [\"accuracy\"]\n",
    "    elif type == \"mood\":\n",
    "        activation, loss, metrics = \"sigmoid\", \"sparse_categorical_crossentropy\", [\"accuracy\"]\n",
    "    else:\n",
    "        activation, loss, metrics = \"softmax\", \"binary_crossentropy\", [\"categorical_accuracy\"]\n",
    "\n",
    "    conv_base = keras.applications.EfficientNetB0(\n",
    "                    include_top = False, \n",
    "                    weights = \"imagenet\",\n",
    "                    drop_connect_rate=0.8)\n",
    "\n",
    "    model = keras.models.Sequential([keras.layers.Conv2D(3,(3,3),padding='same'), \n",
    "                                    conv_base, \n",
    "                                    keras.layers.GlobalAveragePooling2D(),                                 \n",
    "                                    keras.layers.Dense(labels, activation = activation)])\n",
    "    \n",
    "    model.compile(optimizer = \"adam\", loss = loss, metrics = metrics)\n",
    "    return model\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_target(t: Track, genres2labels: Dict[str, int]) -> int:\n",
    "    return genres2labels[t[\"genre\"][\"S\"]]\n",
    "\n",
    "\n",
    "def mood_target(t: Track) -> List[float]:\n",
    "    default: N = {\"N\": 0.5}\n",
    "\n",
    "    acoustic = t.get(\"acoustic\") or default\n",
    "    aggressive = t.get(\"aggressive\") or default\n",
    "    electronic = t.get(\"electronic\") or default\n",
    "    happy = t.get(\"happy\") or default\n",
    "    party = t.get(\"party\") or default\n",
    "    relaxed = t.get(\"relaxed\") or default \n",
    "    sad = t.get(\"sad\") or default\n",
    "    return [acoustic[\"N\"], aggressive[\"N\"], electronic[\"N\"], happy[\"N\"], party[\"N\"], relaxed[\"N\"], sad[\"N\"]]\n",
    "\n",
    "def subgenre_target(t: Track, subgenres: List[str]) -> List[Literal[1, 0]]:\n",
    "    track_subgenres = set([s[\"S\"] for s in t[\"subgenres\"][\"L\"]])\n",
    "    return [1 if sub in track_subgenres else 0 for sub in subgenres]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgenres: ['classical---classical', 'classical---romantic', 'classical---baroque', 'classical---modern', 'classical---opera']\n",
      "['classical---classical', 'classical---romantic', 'classical---baroque', 'classical---modern', 'classical---opera']\n"
     ]
    }
   ],
   "source": [
    "labels: LabelsJson = load_json(LABELS) # type: ignore\n",
    "\n",
    "# Track Genres:\n",
    "genres = list(set([t[\"genre\"][\"S\"] for t in labels[\"tracks\"].values()]))\n",
    "genres2labels = {g: i for i,g in enumerate(genres)}\n",
    "labels2genres = {i: g for i,g in enumerate(genres)}\n",
    "\n",
    "# Track Subgenres: \n",
    "subgenres: Dict[str, List[str]] = load_json(SUBGENERES)\n",
    "subgenre_list = list(subgenres.keys())\n",
    "\n",
    "# Get all subgenres:\n",
    "subgenre_list = get_subgenres(\"classical\", subgenre_list)\n",
    "print(f\"Subgenres: {subgenre_list}\")\n",
    "print(subgenre_list)\n",
    "\n",
    "subgen2labels = {g: i for i,g in enumerate(subgenre_list)}\n",
    "labels2subgen = {i: g for i,g in enumerate(subgenre_list)}\n",
    "\n",
    "target_function = lambda t: subgenre_target(t, subgenre_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 606 + test 130 + val 130 = 866\n",
      "shape: (30, 32, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "mfccs = load_mfccs(subgenre_list, subgenres)\n",
    "random.shuffle(mfccs)\n",
    "\n",
    "train, test, val = train_test_val(mfccs)\n",
    "\n",
    "print(f\"train {len(train)} + test {len(test)} + val {len(val)} = {len(train) + len(test) + len(val)}\" )\n",
    "\n",
    "# Initialize the dataset generators \n",
    "train_generator = TrackDataGen(train, target_function,  labels)\n",
    "test_generator = TrackDataGen(test, target_function,  labels)\n",
    "validation_generator = TrackDataGen(val, target_function,  labels)\n",
    "\n",
    "print(f\"shape: {train_generator[0][0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - ETA: 0s - loss: -97.2415 - accuracy: 0.2554INFO:tensorflow:Assets written to: .\\checkpoint\\assets\n",
      "190/190 [==============================] - 83s 379ms/step - loss: -97.2415 - accuracy: 0.2554 - val_loss: -544.4598 - val_accuracy: 0.1886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build and train the net \n",
    "model = build_network(\"multilabel\", labels=len(subgenre_list),)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data = validation_generator, \n",
    "                    epochs=1,\n",
    "                    callbacks = [stop_callback, checkpoint_callback],\n",
    "                    batch_size = BATCH_SIZE\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgenres: ['classical---classical', 'classical---romantic', 'classical---baroque', 'classical---modern', 'classical---opera']\n",
      "['classical---classical', 'classical---romantic', 'classical---baroque', 'classical---modern', 'classical---opera']\n",
      "train 606 + test 130 + val 130 = 866\n",
      "shape: (30, 32, 130, 1)\n",
      "190/190 [==============================] - ETA: 0s - loss: -64.1000 - accuracy: 0.1246INFO:tensorflow:Assets written to: .\\checkpoint\\assets\n",
      "190/190 [==============================] - 79s 360ms/step - loss: -64.1000 - accuracy: 0.1246 - val_loss: 48.0951 - val_accuracy: 0.0146\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "840059d91b46d815dd2fea4dd332ba7b7b298ef18fb3c7476b6ebc949d933804"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

@misc{onnxruntime,
  title={ONNX Runtime},
  author={ONNX Runtime developers},
  year={2021},
  howpublished={\url{https://onnxruntime.ai/}},
  note={Version: x.y.z}
}
@misc{elastic,
author = {},
title = {Búsqueda gratuita y abierta: Los creadores de Elasticsearch, ELK y Kibana | Elastic},
howpublished = {\url{https://www.elastic.co/es/}},
month = {},
year = {},
note = {(Accessed on 07/04/2022)}
}

 
@book{book_mfcc,
author = {Rao, K. and k e, Manjunath},
year = {2017},
month = {01},
pages = {},
title = {Speech Recognition Using Articulatory and Excitation Source Features},
isbn = {978-3-319-49219-3},
doi = {10.1007/978-3-319-49220-9}
}
 
@inproceedings{mcfee2015librosa,
  title={librosa: Audio and music signal analysis in python},
  author={McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel PW and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  booktitle={Proceedings of the 14th python in science conference},
  volume={8},
  year={2015}
}

@article{Hug2020,
  doi = {10.21105/joss.02174},
  url = {https://doi.org/10.21105/joss.02174},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {52},
  pages = {2174},
  author = {Nicolas Hug},
  title = {Surprise: A Python library for recommender systems},
  journal = {Journal of Open Source Software}
}

@misc{siamese,
  doi = {10.48550/ARXIV.1710.10974},
  
  url = {https://arxiv.org/abs/1710.10974},
  
  author = {Manocha, Pranay and Badlani, Rohan and Kumar, Anurag and Shah, Ankit and Elizalde, Benjamin and Raj, Bhiksha},
  
  keywords = {Sound (cs.SD), Information Retrieval (cs.IR), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Content-based Representations of audio using Siamese neural networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Obscurif67:online,
author = {},
title = {Obscurify Music},
howpublished = {\url{https://obscurifymusic.com/home}},
month = {},
year = {},
note = {(Accessed on 06/20/2022)}
}

@misc{RYM,
author = {},
title = {Welcome! - Rate Your Music},
howpublished = {\url{https://rateyourmusic.com/}},
month = {},
year = {},
note = {(Accessed on 06/20/2022)}
}

@misc{OrganizeYourPlaylist,
author = {},
title = {Organize Your Music},
howpublished = {\url{http://organizeyourmusic.playlistmachinery.com/}},
month = {},
year = {},
note = {(Accessed on 06/20/2022)}
}

@inproceedings{spotify_recommendations,
author = {Jacobson, Kurt and Murali, Vidhya and Newett, Edward and Whitman, Brian and Yon, Romain},
title = {Music Personalization at Spotify},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959120},
doi = {10.1145/2959100.2959120},
abstract = {Spotify is the world's largest on-demand music streaming company, with over 75 million active listeners choosing what to listen to among tens of millions songs. Discovery and personalization is a key part of the experience and critical to the success of the creator and consumer ecosystem. In this talk, we'll discuss the state of our current discovery approaches, such as the Discover Weekly playlist that has already streamed billions of new discoveries and Fresh Finds, a scalable platform for brand new music that focuses suggestions on the long end of the popularity tail. We'll discuss the technologies at scale necessary to distill the information about music from our listeners and the world at large we collect outside of Spotify -- with the massive amounts of user-item activity data we collect every day to create highly personalized music experiences. Entire teams at Spotify focus on understanding both the creator and listener through collaborative filtering, machine learning, DSP and NLP approaches -- we crawl the web for artist information, scan each note in every one of our millions of songs for acoustic signals, and model users' taste through a cluster analysis and in a latent space based on their historical and real-time listening patterns. The data generated by these analyses have ensured our discovery products are precise and help our users enjoy music and media across our entire catalog. We'll dive deep into the workings of Discover Weekly, our marquee personalized playlist which updates weekly and reached 1 billion streams within the first 10 weeks from its release. The technology behind Discover Weekly is powered by a scalable factor analysis of Spotify's over two billion user-generated playlists matched to each user's current listening behavior. We'll discuss its innovative genesis and the challenges and opportunities the system faces a year after its launch. We'll also discuss Spotify's home page, seen by each of our users, currently undergoing vast efforts around personalization to ensure each listener gets a targeted list of playlists, shows and music to select throughout their day. We'll discuss the various similarity metrics, ranking approaches and user modeling we're working on to increase precision and optimize for our users' happiness.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {373},
numpages = {1},
keywords = {recommender system, discover weekly, data pipelines, spotify, machine learning, music personalization, collaborative filtering},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@misc{spotify_docs,
author = {},
title = {Documentation | Spotify for Developers},
howpublished = {\url{https://developer.spotify.com/documentation/}},
month = {},
year = {},
note = {(Accessed on 07/02/2022)}
}

@misc{last.fm_2022,
author = {},
title = {API Docs | Last.fm},
howpublished = {\url{https://www.last.fm/api}},
month = {},
year = {},
note = {(Accessed on 07/02/2022)}
}

@misc{a2022_discogs,
  title = {Discogs: la Base de Datos y el mercado online de la música},
  url = {https://www.discogs.com/},
  urldate = {2022-07-01},
  year = {2022},
  organization = {Discogs.com}
}

@misc{ TDD,
author = "{Wikipedia contributors}",
title = "Test-driven development --- {Wikipedia}{,} The Free Encyclopedia",
year = "2022",
url = "https://en.wikipedia.org/w/index.php?title=Test-driven_development&oldid=1092543030",
note = "[Online; accessed 1-July-2022]"
}

@InProceedings{deeplearn_vs_trad,
author="Lau, Dhevan S.
and Ajoodha, Ritesh",
editor="Yang, Xin-She
and Sherratt, Simon
and Dey, Nilanjan
and Joshi, Amit",
title="Music Genre Classification: A Comparative Study Between Deep Learning and Traditional Machine Learning Approaches",
booktitle="Proceedings of Sixth International Congress on Information and Communication Technology",
year="2022",
publisher="Springer Singapore",
address="Singapore",
pages="239--247",
abstract="Classifying music by their genres has been an ongoing problem in the field of automatic music classification. The use of deep learning models has risen in popularity and as such, this paper provided a comparative study on music genre classification using a deep learning convolutional neural network approach against 5 traditional off-the-shelf classifiers. Feature selection included spectrograms and content-based features. The classifiers were performed on the popular GTZAN dataset and our experiments showed similar prediction results on test data at around 66{\%}.Lau, Dhevan S.Ajoodha, Ritesh",
isbn="978-981-16-2102-4"
}



@misc{spotify,
author = {},
title = {Spotify},
howpublished = {\url{https://spotify.com/}},
month = {},
year = {},
note = {(Accessed on 06/20/2022)}
}


@misc{cnn_vs_human_acc_music,
  doi = {10.48550/ARXIV.1802.09697},
  
  url = {https://arxiv.org/abs/1802.09697},
  
  author = {Dong, Mingwen},
  
  keywords = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{bogdanov2019acousticbrainz,
author = {Dmitry Bogdanov, Alastair Porter, Hendrik Schreiber, Juli\'{a}n Urbano and Sergio Oramas},
booktitle = {International Society for Music Information Retrieval Conference},
pages = {360--367},
title = {{The AcousticBrainz Genre Dataset: Multi-Source, Multi-Level, Multi-Label, and Large-Scale}},
year = {2019}
}

@article{musicbrainz,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Swartz, Aaron},
  biburl = {https://www.bibsonomy.org/bibtex/28dada566d8a99f089dbd8b4d1c42c8a2/dblp},
  ee = {https://www.wikidata.org/entity/Q53998560},
  interhash = {7b429e829d280b7c5d6b1e501c5427d8},
  intrahash = {8dada566d8a99f089dbd8b4d1c42c8a2},
  journal = {IEEE Intelligent Systems},
  keywords = {dblp},
  number = 1,
  pages = {76-77},
  timestamp = {2018-11-15T14:11:22.000+0100},
  title = {MusicBrainz: A Semantic Web Service.},
  url = {http://dblp.uni-trier.de/db/journals/expert/expert17.html#Swartz02},
  volume = 17,
  year = 2002
}

@inproceedings{million_playlist_dataset,
author = {Chen, Ching-Wei and Lamere, Paul and Schedl, Markus and Zamani, Hamed},
title = {Recsys Challenge 2018: Automatic Music Playlist Continuation},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240342},
doi = {10.1145/3240323.3240342},
abstract = {The ACM Recommender Systems Challenge 2018 focused on automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length, the challenge was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the Challenge, Spotify released a dataset of one million user-created playlists, along with associated metadata. Participants could submit their approaches in two tracks, i.e., main and creative tracks, where the former allowed teams to use solely the provided dataset and the latter allowed them to exploit publicly available external data too. In total, 113 teams submitted 1,228 runs in the main track; 33 teams submitted 239 runs in the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was realized by the best team.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {527–528},
numpages = {2},
keywords = {music recommendation systems, recommender systems, automatic playlist continuation, benchmark, challenge, dataset, evaluation},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{OVA_vs_OVO,
title = {One-vs-One classification for deep neural networks},
journal = {Pattern Recognition},
volume = {108},
pages = {107528},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107528},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320303319},
author = {Pornntiwa Pawara and Emmanuel Okafor and Marc Groefsema and Sheng He and Lambert R.B. Schomaker and Marco A. Wiering},
keywords = {Deep learning, Computer vision, Multi-class classification, One-vs-One classification, Plant recognition},
abstract = {For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture, we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme.}
}

@misc{convolution_guide,
  doi = {10.48550/ARXIV.1603.07285},
  
  url = {https://arxiv.org/abs/1603.07285},
  
  author = {Dumoulin, Vincent and Visin, Francesco},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A guide to convolution arithmetic for deep learning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{CNN_architecture,
author = {Mitcheltree, Christopher and Koike, Hideki},
year = {2021},
month = {02},
pages = {},
title = {White-box Audio VST Effect Programming}
}

@article{supriselib,
  doi = {10.21105/joss.02174},
  url = {https://doi.org/10.21105/joss.02174},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {52},
  pages = {2174},
  author = {Nicolas Hug},
  title = {Surprise: A Python library for recommender systems},
  journal = {Journal of Open Source Software}
}

@INPROCEEDINGS{augmentation,
  author={Aguiar, Rafael L. and Costa, Yandre M.G. and Silla, Carlos N.},
  booktitle={2018 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Exploring Data Augmentation to Improve Music Genre Classification with ConvNets}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN.2018.8489166}}

@INPROCEEDINGS{moods_classification,
  author={Laurier, Cyril and Grivolla, Jens and Herrera, Perfecto},
  booktitle={2008 Seventh International Conference on Machine Learning and Applications}, 
  title={Multimodal Music Mood Classification Using Audio and Lyrics}, 
  year={2008},
  volume={},
  number={},
  pages={688-693},
  doi={10.1109/ICMLA.2008.96}}

@misc{demucs,
  doi = {10.48550/ARXIV.1911.13254},
  
  url = {https://arxiv.org/abs/1911.13254},
  
  author = {Défossez, Alexandre and Usunier, Nicolas and Bottou, Léon and Bach, Francis},
  
  keywords = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Music Source Separation in the Waveform Domain},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{million_song_dataset,
  added-at = {2020-03-12T00:00:00.000+0100},
  author = {Bertin-Mahieux, Thierry and Ellis, Daniel P. W. and Whitman, Brian and Lamere, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2911089bee768447cef81a6a55f1a58a3/dblp},
  booktitle = {ISMIR},
  crossref = {conf/ismir/2011},
  editor = {Klapuri, Anssi and Leider, Colby},
  ee = {http://ismir2011.ismir.net/papers/OS6-1.pdf},
  interhash = {41624e24dc1f5aa54aa9d41aeb3bc5ec},
  intrahash = {911089bee768447cef81a6a55f1a58a3},
  isbn = {978-0-615-54865-4},
  keywords = {dblp},
  pages = {591-596},
  publisher = {University of Miami},
  timestamp = {2020-03-13T12:59:57.000+0100},
  title = {The Million Song Dataset.},
  url = {http://dblp.uni-trier.de/db/conf/ismir/ismir2011.html#Bertin-MahieuxEWL11},
  year = 2011
}



@misc{quant_survey,
  doi = {10.48550/ARXIV.2103.13630},
  
  url = {https://arxiv.org/abs/2103.13630},
  
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{LudwigMu18:online,
author = {},
title = {Ludwig Music Dataset (Moods and Subgenres) | Kaggle},
howpublished = {\url{https://www.kaggle.com/datasets/jorgeruizdev/ludwig-music-dataset-moods-and-subgenres?select=mfccs}},
month = {},
year = {},
note = {(Accessed on 06/21/2022)}
}

@misc{GtzanExt16:online,
author = {},
title = {Gtzan Extended Wav | Kaggle},
howpublished = {\url{https://www.kaggle.com/datasets/jorgeruizdev/gtzan-extended-wav/settings}},
month = {},
year = {},
note = {(Accessed on 06/21/2022)}
}

@article{efficientnet,
  doi = {10.48550/ARXIV.1905.11946},
  
  url = {https://arxiv.org/abs/1905.11946},
  
  author = {Tan, Mingxing and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@ARTICLE{gtzan_paper,  author={Tzanetakis, G. and Cook, P.},  journal={IEEE Transactions on Speech and Audio Processing},   title={Musical genre classification of audio signals},   year={2002},  volume={10},  number={5},  pages={293-302},  doi={10.1109/TSA.2002.800560}}

@article{gtzan_analysis,
	doi = {10.1080/09298215.2014.894533},
  
	url = {https://doi.org/10.1080%2F09298215.2014.894533},
  
	year = 2014,
	month = {apr},
  
	publisher = {Informa {UK} Limited},
  
	volume = {43},
  
	number = {2},
  
	pages = {147--172},
  
	author = {Bob L. Sturm},
  
	title = {The State of the Art Ten Years After a State of the Art: Future Research in Music Information Retrieval},
  
	journal = {Journal of New Music Research}
}


@article{efficientnet,
  doi = {10.48550/ARXIV.1905.11946},
  
  url = {https://arxiv.org/abs/1905.11946},
  
  author = {Tan, Mingxing and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
